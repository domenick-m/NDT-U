{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmifsud/miniconda3/envs/ndtu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.data.create_local_t5data import get_trial_data\n",
    "from datasets import get_testing_data\n",
    "from utils_f import get_config\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from datetime import datetime\n",
    "from utils.plot.plot_true_vs_pred_mvmnt import plot_true_vs_pred_mvmnt\n",
    "import torch\n",
    "from utils_f import get_config_from_file, set_seeds, set_device\n",
    "from datasets import get_trial_data, chop, smooth_spikes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import cm, colors\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/honest-sweep-10/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/stellar-sweep-1/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/sweet-serenity-164/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/polar-violet-159/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/neat-sweep-6/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/resilient-sweep-7/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/bumbling-sweep-28/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/clean-energy-185/last.pt'\n",
    "# path = '/home/dmifsud/Projects/NDT-U/runs/train/zesty-sweep-17/last.pt'\n",
    "path = '/home/dmifsud/Projects/NDT-U/runs/train/misunderstood-elevator-1/last.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plots/misunderstood-elevator-1/config.yaml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = path[:path.rindex('/')].split('/')[-1]\n",
    "config = get_config_from_file(path[:path.rindex('/')+1]+'config.yaml')\n",
    "if not os.path.isdir(f\"plots/{name}\"): os.makedirs(f\"plots/{name}\")\n",
    "shutil.copyfile(path[:path.rindex('/')+1]+'config.yaml', f\"plots/{name}/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = get_testing_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seeds(config)\n",
    "# set_device(config, {})\n",
    "# device = torch.device('cuda:0')\n",
    "\n",
    "# model = torch.load(path).to(device)\n",
    "# model.name = name\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# session_csv = pd.read_csv(f'{config.data.dir}/sessions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = copy.deepcopy(datasets[session]) # do not want to run xcorr on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run(dataset, config, model, session, device, cond_list):\n",
    "#     if osp.exists('session_trials/cond_list.pickle'):\n",
    "#         with open('session_trials/cond_list.pickle', 'rb') as handle:\n",
    "#             cond_list = pickle.load(handle)\n",
    "#     else:\n",
    "#         cond_list = None\n",
    "#     session_csv = pd.read_csv(f'{config.data.dir}/sessions.csv')\n",
    "\n",
    "#     if config.data.rem_xcorr: \n",
    "#         corr, corr_chans = dataset.get_pair_xcorr('spikes', threshold=0.2, zero_chans=True)\n",
    "        \n",
    "#     dataset.resample(config.data.bin_size / 1000)\n",
    "#     dataset.smooth_spk(60, name='smth') # for use if we want to take mean and std of smth values\n",
    "\n",
    "#     failed_trials = ~dataset.trial_info['is_successful'] \n",
    "#     center_trials = dataset.trial_info['is_center_target']\n",
    "#     ol_block = session_csv.loc[session_csv['session_id'] == session, 'ol_blocks'].item()\n",
    "#     cl_blocks =  ~dataset.trial_info['block_num'].isin([ol_block]).values.squeeze()\n",
    "\n",
    "#     spks = dataset.data[dataset.data['blockNums'].isin([ol_block]).values.squeeze()].spikes.to_numpy()\n",
    "#     spks_idx = dataset.data[dataset.data['blockNums'].isin([ol_block]).values.squeeze()].spikes.index\n",
    "\n",
    "#     n_channels = dataset.data.spikes.shape[-1]\n",
    "\n",
    "#     n_heldout = int(config.data.heldout_pct * n_channels)\n",
    "#     n_heldin = n_channels - n_heldout\n",
    "#     np.random.seed(config.setup.seed)\n",
    "#     heldout_channels = np.random.choice(n_channels, n_heldout, replace=False)\n",
    "#     heldin_channels = torch.ones(n_channels, dtype=bool)\n",
    "#     heldin_channels[heldout_channels] = False\n",
    "\n",
    "#     chopped_spks = chop(np.array(spks[:, heldin_channels]), 30, 29)\n",
    "#     hi_chopped_spks = torch.Tensor(chopped_spks).to(device)\n",
    "\n",
    "\n",
    "#     names = [session for i in range(hi_chopped_spks.shape[0])]\n",
    "#     with torch.no_grad():\n",
    "#         _, output = model(hi_chopped_spks, names)\n",
    "\n",
    "\n",
    "#     # factors_df = pd.DataFrame(outputs[:, -1, :], index=spks_idx[29:], columns=pd.MultiIndex.from_tuples([('factors', f'{i}') for i in range(output.shape[-1])]))\n",
    "#     factors_df = pd.DataFrame(output[:, -1, :].cpu().numpy(), index=spks_idx[29:], columns=pd.MultiIndex.from_tuples([('factors', f'{i}') for i in range(output.shape[-1])]))\n",
    "#     dataset.data = pd.concat([dataset.data, factors_df], axis=1)\n",
    "\n",
    "#     dataset.smooth_spk(config['data']['smth_std'], signal_type='factors', name='smth')\n",
    "\n",
    "#     ignored_trials = failed_trials | center_trials | cl_blocks\n",
    "#     ignored_trials[1] = True\n",
    "\n",
    "#     trial_data = dataset.make_trial_data(\n",
    "#         align_field='start_time',\n",
    "#         align_range=(0, config.data.trial_len),\n",
    "#         allow_overlap=True,\n",
    "#         ignored_trials= ignored_trials\n",
    "#     )\n",
    "\n",
    "#     trial_data.sort_index(axis=1, inplace=True)\n",
    "#     trial_data['X&Y'] = list(zip(trial_data['targetPos']['x'], trial_data['targetPos']['y']))\n",
    "#     trial_data['condition'] = 0\n",
    "\n",
    "#     if cond_list == None:\n",
    "#         cond_list = list(zip(trial_data['X&Y'].unique(), np.arange(1,9)))\n",
    "#     for xy, id in cond_list:\n",
    "#         indices = trial_data.index[trial_data['X&Y'] == xy]\n",
    "#         trial_data.loc[indices, 'condition'] = id\n",
    "#         print(id, xy)\n",
    "\n",
    "#     factors = []\n",
    "#     for cond_id, trials in trial_data.groupby('condition'):\n",
    "#         for trial_id, trial in trials.groupby('trial_id'):\n",
    "#             factors.append(trial.factors_smth.to_numpy())\n",
    "\n",
    "#     np.save(f'session_factors/{session}.npy', np.array(factors))\n",
    "\n",
    "#     with open(f'session_trials/{session}.pickle', 'wb') as handle:\n",
    "#         pickle.dump(trial_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#     if not osp.exists('session_trials/cond_list.pickle'):  \n",
    "#         with open(f'session_trials/cond_list.pickle', 'wb') as handle:\n",
    "#             pickle.dump(cond_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_list = [\n",
    "   't5.2021.05.05',\n",
    "   't5.2021.05.17',\n",
    "   't5.2021.05.19',\n",
    "   't5.2021.05.24',\n",
    "   # 't5.2021.05.26',\n",
    "   't5.2021.06.02',\n",
    "   't5.2021.06.04',\n",
    "   't5.2021.06.07',\n",
    "   't5.2021.06.23',\n",
    "   't5.2021.06.28',\n",
    "   't5.2021.06.30',\n",
    "   't5.2021.07.07',\n",
    "   't5.2021.07.08',\n",
    "   't5.2021.07.12',\n",
    "   't5.2021.07.14',\n",
    "   't5.2021.07.19',\n",
    "   't5.2021.07.21',   \n",
    "]   \n",
    "factors_list = [\n",
    "   'session_factors/t5.2021.05.05_.npy',\n",
    "   'session_factors/t5.2021.05.17_.npy',\n",
    "   'session_factors/t5.2021.05.19_.npy',\n",
    "   'session_factors/t5.2021.05.24_.npy',\n",
    "   # 'session_factors/t5.2021.05.26_.npy',\n",
    "   'session_factors/t5.2021.06.02_.npy',\n",
    "   'session_factors/t5.2021.06.04_.npy',\n",
    "   'session_factors/t5.2021.06.07_.npy',\n",
    "   'session_factors/t5.2021.06.23_.npy',\n",
    "   'session_factors/t5.2021.06.28_.npy',\n",
    "   'session_factors/t5.2021.06.30_.npy',\n",
    "   'session_factors/t5.2021.07.07_.npy',\n",
    "   'session_factors/t5.2021.07.08_.npy',\n",
    "   'session_factors/t5.2021.07.12_.npy',\n",
    "   'session_factors/t5.2021.07.14_.npy',\n",
    "   'session_factors/t5.2021.07.19_.npy',\n",
    "   'session_factors/t5.2021.07.21_.npy',   \n",
    "]   \n",
    "trials_list = [\n",
    "   'session_trials/t5.2021.05.05_.pickle',\n",
    "   'session_trials/t5.2021.05.17_.pickle',\n",
    "   'session_trials/t5.2021.05.19_.pickle',\n",
    "   'session_trials/t5.2021.05.24_.pickle',\n",
    "   # 'session_trials/t5.2021.05.26_.pickle',\n",
    "   'session_trials/t5.2021.06.02_.pickle',\n",
    "   'session_trials/t5.2021.06.04_.pickle',\n",
    "   'session_trials/t5.2021.06.07_.pickle',\n",
    "   'session_trials/t5.2021.06.23_.pickle',\n",
    "   'session_trials/t5.2021.06.28_.pickle',\n",
    "   'session_trials/t5.2021.06.30_.pickle',\n",
    "   'session_trials/t5.2021.07.07_.pickle',\n",
    "   'session_trials/t5.2021.07.08_.pickle',\n",
    "   'session_trials/t5.2021.07.12_.pickle',\n",
    "   'session_trials/t5.2021.07.14_.pickle',\n",
    "   'session_trials/t5.2021.07.19_.pickle',\n",
    "   'session_trials/t5.2021.07.21_.pickle',   \n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in session_list:\n",
    "#     !python run_model.py $i $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factors = []\n",
    "# for session in config.data.pretrain_sessions:    \n",
    "#     for cond_id, trials in trialized_data[session].groupby('condition'):\n",
    "#         for trial_id, trial in trials.groupby('trial_id'):\n",
    "#             factors.append(trial.factors_smth.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147400, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=7, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "factors = []\n",
    "for i in factors_list:    \n",
    "    factors.append(np.load(i))\n",
    "\n",
    "factors = np.concatenate(factors, 0)\n",
    "fs = factors.shape\n",
    "print(fs)\n",
    "# factors = factors.reshape((fs[0] * fs[1], fs[2]))\n",
    "pca = PCA(n_components=7)\n",
    "# pca = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=7))])\n",
    "pca.fit(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialized_data = {}\n",
    "for sess, trials in zip(session_list, trials_list):\n",
    "    with open(trials, 'rb') as handle:\n",
    "        trialized_data[sess] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t5.2021.05.05', 't5.2021.05.17', 't5.2021.05.19', 't5.2021.05.24', 't5.2021.06.02', 't5.2021.06.04', 't5.2021.06.07', 't5.2021.06.23', 't5.2021.06.28', 't5.2021.06.30', 't5.2021.07.07', 't5.2021.07.08', 't5.2021.07.12', 't5.2021.07.14', 't5.2021.07.19', 't5.2021.07.21'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trialized_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memory-bg2\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dmifsud/Projects/NDT-U/wandb/run-20221110_112643-2dj46dy6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emory-bg2/High%20Dropout%20Sweep/runs/2dj46dy6\" target=\"_blank\">all_sessions_aligned</a></strong> to <a href=\"https://wandb.ai/emory-bg2/High%20Dropout%20Sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/emory-bg2/High%20Dropout%20Sweep/runs/2dj46dy6?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2af99c3a30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='High Dropout Sweep', name='all_sessions_aligned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COND AVG\n",
    "    \n",
    "fig = go.Figure()\n",
    "for session in session_list:    \n",
    "    for cond_id, trials in trialized_data[session].groupby('condition'):\n",
    "        avg_trials = []\n",
    "        for trial_id, trial in trials.groupby('trial_id'):\n",
    "            avg_trials.append(pca.transform(trial.factors_smth))\n",
    "        avg_trials = np.array(avg_trials).mean(0)\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=avg_trials[:, 0], \n",
    "                y=avg_trials[:, 1], \n",
    "                z=avg_trials[:, 2],\n",
    "                mode='lines',\n",
    "                line=dict(color=f'{colors.rgb2hex(cm.tab10(cond_id))}'),\n",
    "            )\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    width=430,\n",
    "    height=410,\n",
    "    autosize=False,\n",
    "    showlegend=False,\n",
    "    title={\n",
    "        'text': \"Condition Averaged PCs\",\n",
    "        'y':0.96,\n",
    "        'yanchor': 'bottom'\n",
    "    },\n",
    "    scene=dict(\n",
    "        xaxis_showspikes=False,\n",
    "        yaxis_showspikes=False,\n",
    "        zaxis_showspikes=False,\n",
    "        xaxis_title=\"PC1\",\n",
    "        yaxis_title=\"PC2\",\n",
    "        zaxis_title=\"PC3\",\n",
    "        camera=dict(\n",
    "            center=dict(\n",
    "                x=0.065,\n",
    "                y=0.0,\n",
    "                z=-0.075,\n",
    "                # z=-0.12,\n",
    "            ),\n",
    "            eye=dict(\n",
    "                x=1.3, \n",
    "                y=1.3, \n",
    "                z=1.3\n",
    "            )\n",
    "        ),\n",
    "        aspectratio = dict( x=1, y=1, z=1 ),\n",
    "        aspectmode = 'manual'\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(r=0, l=0, b=0, t=20))\n",
    "config2 = {'displayModeBar': False}\n",
    "html_string = fig.to_html(config=config2)\n",
    "\n",
    "wandb.log({'PC_plots_cond_avg': wandb.Html(html_string, inject=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE TRIAL\n",
    "\n",
    "fig = go.Figure()\n",
    "for session in session_list:    \n",
    "    for cond_id, trials in trialized_data[session].groupby('condition'):\n",
    "        for trial_id, trial in trials.groupby('trial_id'):\n",
    "            pc_factors = pca.transform(trial.factors_smth)\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=pc_factors[:, 0], \n",
    "                    y=pc_factors[:, 1], \n",
    "                    z=pc_factors[:, 2],\n",
    "                    mode='lines',\n",
    "                    line=dict(color=f'{colors.rgb2hex(cm.tab10(cond_id))}'),\n",
    "                )\n",
    "            )\n",
    "\n",
    "fig.update_layout(\n",
    "    width=430,\n",
    "    height=410,\n",
    "    autosize=False,\n",
    "    showlegend=False,\n",
    "    title={\n",
    "        'text': \"Single Trial PCs\",\n",
    "        'y':0.96,\n",
    "        'yanchor': 'bottom'\n",
    "    },\n",
    "    scene=dict(\n",
    "        xaxis_showspikes=False,\n",
    "        yaxis_showspikes=False,\n",
    "        zaxis_showspikes=False,\n",
    "        xaxis_title=\"PC1\",\n",
    "        yaxis_title=\"PC2\",\n",
    "        zaxis_title=\"PC3\",\n",
    "        camera=dict(\n",
    "            center=dict(\n",
    "                x=0.065,\n",
    "                y=0.0,\n",
    "                z=-0.075,\n",
    "                # z=-0.12,\n",
    "            ),\n",
    "            eye=dict(\n",
    "                x=1.3, \n",
    "                y=1.3, \n",
    "                z=1.3\n",
    "            )\n",
    "        ),\n",
    "        aspectratio = dict( x=1, y=1, z=1 ),\n",
    "        aspectmode = 'manual'\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(r=0, l=0, b=0, t=20))\n",
    "config2 = {'displayModeBar': False}\n",
    "html_string = fig.to_html(config=config2)\n",
    "\n",
    "wandb.log({'PC_plots_single_trials': wandb.Html(html_string, inject=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">all_sessions_aligned</strong>: <a href=\"https://wandb.ai/emory-bg2/High%20Dropout%20Sweep/runs/2dj46dy6\" target=\"_blank\">https://wandb.ai/emory-bg2/High%20Dropout%20Sweep/runs/2dj46dy6</a><br/>Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221110_112643-2dj46dy6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ndtu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59cd4eb5f467b822ded03db0cb076f5e7005d54ea28457b2083fbdb9b80fb7dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
